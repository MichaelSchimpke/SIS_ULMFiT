{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<style>\n",
    "//change background settings {}\n",
    "div.slide-background {\n",
    "\tborder-bottom: 30px crimson solid;\n",
    "}\n",
    "</style>\n",
    "\n",
    "# Universal Language Model Fine-Tuning (ULMFiT)\n",
    "## State-of-the-Art in Text Analysis\n",
    "<br> <br>\n",
    "Sandra Faltl, Michael Schimpke & Constantin Hackober"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Table of Contents\n",
    "\n",
    "**1. Introduction**\n",
    "\n",
    "**2. General-Domain Language Model Pretraining**\n",
    "    1. AWD-LSTM\n",
    "    2. Implementation\n",
    "**3. Target Task Language Model Fine-Tuning**\n",
    "    1. Model Overview\n",
    "    2. Fine-Tuning Methods\n",
    "    3. Implementation\n",
    "**4. Target Task Classifier**\n",
    "    1. Classifier\n",
    "    2. Implementation\n",
    "    3. Results\n",
    "**5. Our Model Extension**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "dfTrn=pd.read_csv('C:/Sandra/Dokumente_Sandra/ULMFiT/lm/train.csv', header=None)\n",
    "tokTrn = np.load('C:/Sandra/Dokumente_Sandra/ULMFiT/lm/tokTrn.npy')\n",
    "trnLm = np.load('C:/Sandra/Dokumente_Sandra/ULMFiT/lm/trnIds.npy')\n",
    "valLm = np.load('C:/Sandra/Dokumente_Sandra/ULMFiT/lm/valIds.npy')\n",
    "with open('C:/Sandra/Dokumente_Sandra/ULMFiT/lm/itos.pkl', 'rb') as pickle_file:\n",
    "    itos = pickle.load(pickle_file)\n",
    "stoi = collections.defaultdict(lambda:0, \n",
    "                               {v:k for k,v in enumerate(itos)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@USAirways has the WORST customer service and their flights are always delayed'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrn.iloc[40,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " 'xbos',\n",
       " '@usairways',\n",
       " 'has',\n",
       " 'the',\n",
       " 't_up',\n",
       " 'worst',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'and',\n",
       " 'their',\n",
       " 'flights',\n",
       " 'are',\n",
       " 'always',\n",
       " 'delayed']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokTrn[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 20, 103, 7, 8, 187, 66, 52, 18, 217, 80, 48, 352, 92]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trnLm[40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['back', 'or', 'had', 'has', 'as']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itos[100:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'back': 100, 'or': 101, 'had': 102, 'has': 103, 'as': 104}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: stoi[k] for k in list(stoi)[100:105]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "em_sz, nh, nl = 400, 1150, 3\n",
    "\n",
    "# Load WikiText103 itos\n",
    "with open('C:/Sandra/Dokumente_Sandra/ULMFiT/wt103/itos_wt103.pkl', 'rb') as pickle_file:\n",
    "    itosWiki = pickle.load(pickle_file)\n",
    "\n",
    "# Calculate WikiText103 stoi    \n",
    "stoiWiki = collections.defaultdict(lambda:-1, {v:k for k,v in enumerate(itosWiki)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['college', '17', 'construction', 'should', 'award']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itosWiki[500:505]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'college': 500, '17': 501, 'construction': 502, 'should': 503, 'award': 504}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: stoiWiki[k] for k in list(stoiWiki)[500:505]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(238462, 400)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load WikiText103 weights\n",
    "wgts = torch.load('C:/Sandra/Dokumente_Sandra/ULMFiT/wt103/fwd_wt103.h5', \n",
    "                  map_location=lambda storage, loc: storage)\n",
    "encWgts = wgts['0.encoder.weight'].numpy()\n",
    "encWgts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4409"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate mean of weights\n",
    "rowM = np.mean(encWgts, axis = 0)\n",
    "\n",
    "# Create new embedding matrix\n",
    "newWm = np.zeros((len(itos), em_sz), dtype=np.float32)\n",
    "\n",
    "for i,w in enumerate(itos):\n",
    "    r = stoiWiki[w]\n",
    "    newWm[i] = encWgts[r] if r>=0 else rowM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4409, 400])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save new weights\n",
    "wgts['0.encoder.weight'] = T(newWm)\n",
    "wgts['0.encoder.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "bptt = 70\n",
    "bs = 64\n",
    "\n",
    "# Create data loader\n",
    "trnDl = LanguageModelLoader(np.concatenate(trnLm), bs, bptt)\n",
    "valDl = LanguageModelLoader(np.concatenate(valLm), bs, bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize LanguageModelData class\n",
    "md = LanguageModelData('C:/Sandra/Dokumente_Sandra/ULMFiT', 1, len(itos), trnDl, valDl, \n",
    "                       bs=bs, bptt=bptt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Set regularization and optimization parameters and build architecture\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "drops = np.array([0.25, 0.1, 0.2, 0.02, 0.15])*0.7\n",
    "learner = md.get_model(opt_fn, em_sz, nh, nl, \n",
    "                       dropouti = drops[0], dropout=drops[1], wdrop=drops[2], \n",
    "                       dropoute = drops[3], dropouth=drops[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Language Model Overview\n",
    "\n",
    "Our classifying model consists of three steps. First, we pretrain a language model based on the Wikitext-103 dataset, so that our model can learn the basics of language on this big general dataset. We profit from the proper grammar and style of Wikipedia, which does not apply to our Twitter dataset. \n",
    "...\n",
    "\n",
    "After matching the wikipedia dataset to our vocabulary (<i><b> nach Sandras Matching mit rowmeans Twitter etc</b></i>), we train the model on our target-task Twitter dataset. At this point, we face one of the key problems of transfer learning, which is catastrophic forgetting of pretrained knowledge. To prevent this accident, we apply a bunch of methods here in order to be really able to transfer our language knowledge from the Wikitext model. \n",
    "\n",
    "1. Freezing\n",
    "\n",
    "We are starting to train our model with the 3 pretrained LSTM layers. But as we shortened our vocabulary and also added new words, part of our embedding and softmax layer is untrained with initial random weights (row means). If we now train the entire model, we risk catastrophic forgetting in our 3 LSTM layers due to the untrained embedding and softmax layer. So in a first step, we freeze the weights of LSTM layers and train the model in one epoch. Only the embedding and softmax parameters are trained.\n",
    "For the second step all layers are unfrozen that also the LSTM layers can now adapt to the new dataset.\n",
    "\n",
    "2. Discriminative Fine-Tuning\n",
    "\n",
    "As already stated, we apply this form suggested by Howard and Ruder due to the complexity of text data. <i><b>Yosinski et al. 2014</b></i> found out, this architecture is capable of capturing different types of information at every level, starting with general information on the first layer and growing more and more specific on every further layer. In the text data context, the first layer might catch information like basic sentence structure, while the next ones dig deeper into word meanings.\n",
    "\n",
    "We address this issue by discriminating the learning rate among each layer. Starting with a small learning rate in the first LSTM, it rises through the layers due to the rising amount of acquired knowledge and information complexity.\n",
    "\n",
    "3. Learning Rate Schedule\n",
    "\n",
    "In 2015, Leslie Smith first introduced cyclical triangular learning rates (<i><b>Smith 2015</b></i>). He tested it on the CIFAR and the ImageNet Dataset with various algorithms and compared it to standard learning rates. The approach appeared to boost accuracy in some of the cases, but it never downgraded the results in others. For the ULMFiT model, Howard and Ruder modify this approach to their text dataset by taking a one-cycle learning rate schedule with a short steep increase and a long decay period, which they call <b>slanted triangular learning rate</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Language Model Overview\n",
    "\n",
    "<img src=\"images/Modell_Overview.png\" width=\"1600\" style=\"float:center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will explain the model by channeling a tweet through it step by step. The tweet is called \"My flight was on\" and will be the basis for our language model prediction.\n",
    "\n",
    "The input words are matched with their token ID in the vocabulary of our dataset. It is then one-hot encoded to a $n \\times 1 \\times vs$ tensor, where $n = 4$ representing the length of the input sentence and $vs = 4409$ as length of the vocabulary of our target dataset.\n",
    "\n",
    "Using this sentence as input, our model knows each word's position in the sentence. \n",
    "\n",
    "<b>Embedding Layer</b>\n",
    "In the first layer, the Embedding Layer, words are translated into latent features. As Howard and Ruder provide the results of the time-intensive pretrained Wikipedia language model, we have to stick to their model architecture. They use an embedding size of 400, an LSTM structure of 3 stacked AWD-LSTMs with 1150 hidden activations.\n",
    "\n",
    "In the embedding layer, each vector (representing one word) of our input tensor is multiplied with the pretrained embedding matrix of size $4409 \\times 400$, yielding a $4 \\times 1 \\times 4409$ tensor. This tensor is then the input to our first LSTM.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Language Model Overview\n",
    "\n",
    "<img src=\"images/Modell_Detail_0.png\" width=\"800\" style=\"float:center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Language Model Overview\n",
    "\n",
    "<img src=\"images/Modell_Detail_1.png\" width=\"1600\" style=\"float:center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Language Model Overview\n",
    "\n",
    "<img src=\"images/Modell_Detail_2.png\" width=\"1600\" style=\"float:center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Language Model Overview\n",
    "\n",
    "<img src=\"images/Modell_Detail_3.png\" width=\"1600\" style=\"float:center;\"/>\n",
    "/Users/michaelschimpke/Documents/Präsentation/images/LR_find_LM.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.1 Language Model Overview\n",
    "\n",
    "\n",
    "<img src=\"images/Modell_Detail_4.png\" width=\"1600\" style=\"float:center;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/Modell_Overview_LM_Finetuning.png\" width=\"900\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2 Fine-Tuning Methods\n",
    "\n",
    "<img src=\"images/Overview_LM_FT.png\" width=\"900\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2 Fine-Tuning Methods: Freezing\n",
    "<html>\n",
    "<img src=\"images/Freezing_LM.png\" width=\"350\" style=\"float: right;\"/>\n",
    "\n",
    "<p style=\"margin-top: 22%\"><font size=4>\n",
    "Training all layers at once risks catastrophic forgetting</font></p>\n",
    "<p style=\"margin-top: 3%\"><font size=4>\n",
    "First epoch only updates the weights in the last layer</font></p>\n",
    "<p style=\"margin-top: 3%\"><font size=4>\n",
    "From second period all layers are unfrozen</font></p>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2 Fine-Tuning Methods: Discriminative Fine-Tuning\n",
    "\n",
    "<html>\n",
    "<img src=\"images/Discriminative_FT.png\" width=\"350\" style=\"float: right;\"/>  \n",
    "\n",
    "<p style=\"margin-top: 22%\"><font size=4>\n",
    "    <b>Stochastic gradient descent:</b><br>\n",
    "$\n",
    "\\begin{align}    \n",
    "\\theta_{t} = \\theta_{t-1} - \\eta \\times \\nabla_{\\theta} J(\\theta)\n",
    "\\end{align}\n",
    "$\n",
    "    </font></p>   \n",
    "<p style=\"margin-top: 3%\"><font size=4>\n",
    "<b> Discriminative fine-tuning:</b><br>\n",
    "$\n",
    "\\begin{align}    \n",
    "\\theta_{t}^{l} = \\theta_{t-1}^{l} - \\eta^{l} \\times \\nabla_{\\theta^{l}} J(\\theta)\n",
    "\\end{align}\n",
    "$\n",
    "</font></p>  \n",
    "</html>\n",
    " \n",
    "We are using a stacked model structure containing three LSTM hidden layers. We apply this form suggested by Howard and Ruder due to the complexity of text data. As <i><b>Yosinski et al. 2014</b></i> found out, this architecture is capable of capturing different types of information at every level, starting with general information on the first layer and growing more and more specific on every further layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3.2 Fine-Tuning Methods: Learning Rate Schedule\n",
    "\n",
    "<html>\n",
    "<img src=\"images/Slanted_Triangular_Learning_Rates.png\" width=\"350\" style=\"float: right;\"/>  \n",
    "\n",
    "<p style=\"margin-top: 22%\"><font size=4>\n",
    "<b>Cyclical Learning Rates</b> introduced in 2015</font></p>\n",
    "<p style=\"margin-top: 3%\"><font size=4>\n",
    "Application of slanted one-cycle learning rate (<b>Slanted triangular learning rate</b>)</font></p>\n",
    "</html>\n",
    "\n",
    "In 2015, Leslie Smith first introduced cyclical triangular learning rates (<i><b>Smith 2015</b></i>). He tested it on the CIFAR and the ImageNet Dataset with various algorithms and compared it to standard learning rates. The approach appeared to boost accuracy in some of the cases, but it never downgraded the results in others. For the ULMFiT model, Howard and Ruder modify this approach to their text dataset by taking a one-cycle learning rate schedule with a short steep increase and a long decay period, which they call <b>slanted triangular learning rate</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "## Fine-Tuning the Language Model\n",
    "\n",
    "#Freeze all the layers but the last\n",
    "learner.freeze_to(-1)\n",
    "\n",
    "#Load model weights, set initial learning rate and weight decay\n",
    "learner.model.load_state_dict(wgts)\n",
    "lr=1e-3\n",
    "wd = 1e-7\n",
    "\n",
    "learner.fit(lr, 1, wds=wd, use_clr=(32,2), cycle_len=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table style=\"width:100%\" align=\"center\">\n",
    "  <tr>\n",
    "    <th>LM Fine-Tuning</th>\n",
    "    <th></th> \n",
    "    <th></th>\n",
    "    <th></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>epoch</th>\n",
    "    <th>training loss</th>\n",
    "    <th>validation loss</th> \n",
    "    <th>accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>6.240857</td> \n",
    "    <td>5.909655</td>\n",
    "    <td>0.118566</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Unfreeze all layers\n",
    "learner.unfreeze()\n",
    "\n",
    "#Find good learning rate\n",
    "learner.lr_find(lr/1000)\n",
    "\n",
    "#Plot Loss depending on learning rates\n",
    "learner.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<img src=\"images/LR_find_LM.png\" width=\"400\" style=\"float: left\"/>  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Set new learning rate according to plot\n",
    "lr = 1e-2\n",
    "\n",
    "#Setting array for discriminative fine-tuning\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lmr**3), lr/(lrm**2), lr/lrm, lr])\n",
    "\n",
    "learner.fit(lrs, 1, wds=wd, use_clr=(20,10), cycle_len=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table style=\"width:100%\" align=\"center\">\n",
    "  <tr>\n",
    "    <th>LM Fine-Tuning</th>\n",
    "    <th></th> \n",
    "    <th></th>\n",
    "    <th></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>epoch</th>\n",
    "    <th>training loss</th>\n",
    "    <th>validation loss</th> \n",
    "    <th>accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>4.351183</td> \n",
    "    <td>3.914363</td>\n",
    "    <td>0.274198</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>3.938936</td> \n",
    "    <td>3.795009</td>\n",
    "    <td>0.288417</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>3.681302</td> \n",
    "    <td>3.757219</td>\n",
    "    <td>0.292195</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"images/Encoder_Cutout.png\" width=\"800\" style=\"float: center;\"/>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Save encoder for classifier model\n",
    "learner.save_encoder(lm2)\n",
    "\n",
    "#Plot Losses over Time\n",
    "learner.sched.plot_loss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/Plot_loss_LM.png\" width=\"400\" style=\"float: left\"/>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Classifier\n",
    "\n",
    "<img src=\"images/Inhalt_Classifier.png\" width=\"900\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Classifier\n",
    "\n",
    "<img src=\"images/Modell_Overview_Classifier.png\" width=\"800\" align=\"mid\" style=\"margin-top:50px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Classifier \n",
    "<img src=\"images/Modell_Detail_Concat_Pooling.png\" width=\"800\" align=\"mid\" style=\"margin-top:50px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Classifier: Concat Pooling\n",
    "\n",
    "<img src=\"images/Modell_Overview_Concat_Pooling.png\" width=\"800\" align=\"mid\" style=\"margin-top:50px\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Classifier: Concat Pooling \n",
    "\n",
    "<ul>\n",
    "    <p>\n",
    "    <font size=\"3\"><b>H</b> = {<b>h</b><sub>1</sub>,...,<b>h</b><sub>T</sub>} :</font><br></p>\n",
    "    <p><font size=\"3\" style=\"margin-left:50px; margin-bottom:200px\"><b>h</b><sub>c</sub> = [<b>h</b><sub>T</sub>, maxpool(<b>H</b>), meanpool(<b>H</b>)]</font></p>\n",
    "    <img src=\"images/Concat_Pooling.png\" width=\"800\" align=\"mid\" style=\"margin-top:50px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Classifier \n",
    "\n",
    "<img src=\"images/Modell_Overview_Classifier.png\" width=\"800\" align=\"mid\" style=\"margin-top:50px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Get encoded Tweets\n",
    "itos = pickle.load((lmPath/'itos.pkl').open('rb'))\n",
    "stoi = collections.defaultdict(lambda:0, {v:k for k,v in \n",
    "                                          enumerate(itos)})\n",
    "\n",
    "trn_clas = np.array([[stoi[o] for o in p] for p in tok_trn])\n",
    "val_clas = np.array([[stoi[o] for o in p] for p in tok_val])\n",
    "\n",
    "trn_ds = TextDataset(trnClas, trnLabels)\n",
    "val_ds = TextDataset(valClas, valLabels)\n",
    "\n",
    "md = ModelData('gdrive/My Drive/Colab neu', trn_clas, val_clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Set Hyperparameters \n",
    "bptt,em_sz,nh,nl = 70,400,1150,3\n",
    "vs = len(itos)\n",
    "opt_fn = partial(optim.Adam, betas=(0.8, 0.99))\n",
    "bs = 64\n",
    "c = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Set Dropouts \n",
    "dps = np.array([0.4,0.5,0.05,0.3,0.4])*0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Define our Classifier Model \n",
    "def get_rnn_classifier(bptt, max_seq, n_class, n_tok, emb_sz, n_hid,\n",
    "                       n_layers, pad_token, layers, drops, bidir=False,\n",
    "                       dropouth=0.3, dropouti=0.5, dropoute=0.1, wdrop=0.5):\n",
    "    \n",
    "    rnn_enc = MultiBatchRNN(bptt, max_seq, n_tok, emb_sz, n_hid, n_layers, \n",
    "                            pad_token=pad_token, bidir=bidir,\n",
    "                            dropouth=dropouth, dropouti=dropouti, \n",
    "                            dropoute=dropoute, wdrop=wdrop)\n",
    "    \n",
    "    return SequentialRNN(rnn_enc, PoolingLinearClassifier(layers, drops))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Classifier Model \n",
    "m = get_rnn_classifier(bptt, 1000, c, vs, emb_sz=em_sz, n_hid=nh, \n",
    "                      n_layers=nl, pad_token=1,\n",
    "                      layers=[em_sz*3, 50, c], drops=[dps[4], 0.1],\n",
    "                      dropouti=dps[0], wdrop=dps[1],        \n",
    "                      dropoute=dps[2], dropouth=dps[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Classifier\n",
    "\n",
    "<img src=\"images/Modell_Overview_Classifier.png\" width=\"800\" align=\"mid\" style=\"margin-top:50px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Create a Learner \n",
    "learn = RNN_Learner(md, TextModel(to_gpu(m)), opt_fn=opt_fn)\n",
    "learn.clip = .25\n",
    "learn.metrics = [accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Set learning rate schedule\n",
    "lr = 3e-3\n",
    "lrm = 2.6\n",
    "lrs = np.array([lr/(lrm**4), lr/(lrm**3), lr/(lrm**2), lr/lrm, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#Set weight decay\n",
    "wd = 1e-7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.1 Classifier: Gradual Unfreezing \n",
    "\n",
    "<img src=\"images/Gradual_Unfreezing.png\" width=\"900\" align=\"left\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Load encoder from language model\n",
    "learn.load_encoder('lm2_enc')\n",
    "\n",
    "#Unfreeze last layer\n",
    "learn.freeze_to(-1)\n",
    "learn.lr_find(lrs/1000)\n",
    "\n",
    "#Plot learning schedule\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<img src=\"images/learn.sched(-1).png\" width=\"400\" align=\"left\" style=\"margin-top:50px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table style=\"width:100%\" align=\"center\">\n",
    "  <tr>\n",
    "    <th>Classifier Fine-Tuning</th>\n",
    "    <th></th> \n",
    "    <th></th>\n",
    "    <th></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>epoch</th>\n",
    "    <th>training loss</th>\n",
    "    <th>validation loss</th> \n",
    "    <th>accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>0.686093</td> \n",
    "    <td>0.521651</td>\n",
    "    <td>0.792782 </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Unfreeze both Classifier Layers\n",
    "learn.freeze_to(-2)\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=1, use_clr=(8,3))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table style=\"width:100%\" align=\"center\">\n",
    "  <tr>\n",
    "    <th>Classifier Fine-Tuning</th>\n",
    "    <th></th> \n",
    "    <th></th>\n",
    "    <th></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>epoch</th>\n",
    "    <th>training loss</th>\n",
    "    <th>validation loss</th> \n",
    "    <th>accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>0.659148</td> \n",
    "    <td>0.490926</td>\n",
    "    <td>0.806697</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#Unfreeze all Layers\n",
    "learn.unfreeze()\n",
    "learn.fit(lrs, 1, wds=wd, cycle_len=5, use_clr=(32,10))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<table style=\"width:100%\" align=\"center\">\n",
    "  <tr>\n",
    "    <th>LM Fine-Tuning</th>\n",
    "    <th></th> \n",
    "    <th></th>\n",
    "    <th></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>epoch</th>\n",
    "    <th>training loss</th>\n",
    "    <th>validation loss</th> \n",
    "    <th>accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>0.657155</td> \n",
    "    <td>0.4779 </td>\n",
    "    <td>0.818361 </td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>0.602397</td> \n",
    "    <td>0.483012</td>\n",
    "    <td>0.825041</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>3</td>\n",
    "    <td>0.556655</td> \n",
    "    <td>0.456581</td>\n",
    "    <td>0.829689 </td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>4</td>\n",
    "    <td>0.504563</td> \n",
    "    <td>0.462918</td>\n",
    "    <td>0.831911</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>5</td>\n",
    "    <td>0.468433</td> \n",
    "    <td>0.456508</td>\n",
    "    <td>0.830113</td>\n",
    "  </tr>\n",
    "    <tr>\n",
    "    <td>6</td>\n",
    "    <td>0.45835</td> \n",
    "    <td>0.454914</td>\n",
    "    <td>0.833168</td>\n",
    " \n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "learn.sched.plot_loss()\n",
    "#Insert Graph\n",
    "\n",
    "learn.save('clas_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "<img src=\"images/learn.sched_final.png\" width=\"400\" style=\"float: left\"/>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 4.3 Results: Twitter Airline Sentiment Classification    \n",
    "<img src=\"images/kaggle_logo.png\" width=\"250\" style=\"float: right\"> \n",
    "\n",
    "<ul>\n",
    "    <br><br>\n",
    "    <li>Support Vector Machine (SVM) - 78.5%</li>\n",
    "    <li>bag-of-words SVM - 78.5%</li>\n",
    "    <li>Deep Learning Model with Dropouts in Keras - 77.9%</li>\n",
    "    <li>Our result - <b>83.9%</b></li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5 Our Model Extension: Overview\n",
    "<img src=\"images/addition.png\" width=\"900\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 5 Our Model Extension: Results - Classifier\n",
    "\n",
    "<table style=\"width:45%\" align=\"left\">\n",
    "  <tr>\n",
    "    <th>ULMFiT - Original</th>\n",
    "    <th></th> \n",
    "    <th></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>training loss</th>\n",
    "    <th>validation loss</th> \n",
    "    <th>accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0.458</td>\n",
    "    <td>0.455</td> \n",
    "    <td>0.833</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<table style=\"width:45%\" style=\"float: right\">\n",
    "  <tr>\n",
    "    <th>ULMFiT - Adapted</th>\n",
    "    <th></th> \n",
    "    <th></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>training loss</th>\n",
    "    <th>validation loss</th> \n",
    "    <th>accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>0.522 </td>\n",
    "    <td>0.449</td> \n",
    "    <td>0.835</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Interesting Observation in Respective LMs\n",
    "\n",
    "<table style=\"width:45%\" align=\"left\">\n",
    "  <tr>\n",
    "    <th>ULMFiT - Original</th>\n",
    "    <th></th> \n",
    "    <th></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>training loss</th>\n",
    "    <th>validation loss</th> \n",
    "    <th>accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>6.241</td>\n",
    "    <td>4.977</td> \n",
    "    <td>0.119</td>\n",
    "  </tr>\n",
    "</table>\n",
    "\n",
    "<table style=\"width:45%\" style=\"float: right\">\n",
    "  <tr>\n",
    "    <th>ULMFiT - Adapted</th>\n",
    "    <th></th> \n",
    "    <th></th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>training loss</th>\n",
    "    <th>validation loss</th> \n",
    "    <th>accuracy</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>4.977</td>\n",
    "    <td>4.628</td> \n",
    "    <td>0.246</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
