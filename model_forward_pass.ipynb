{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "#!pip install fastai==0.7.0\n",
    "#!pip install torch\n",
    "import fastai\n",
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.core import *\n",
    "from fastai.model import fit\n",
    "from fastai.dataset import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "from fastai.text import *\n",
    "\n",
    "import dill as pickle\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en\n",
    "import spacy\n",
    "import html\n",
    "\n",
    "import pickle\n",
    "import collections\n",
    "#!pip install nbimporter\n",
    "import nbimporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing Jupyter notebook from functions.ipynb\n"
     ]
    }
   ],
   "source": [
    "#Das kommt ganz am Anfang, mit kleinem Text dass wir um den Code schlank zu halten einige Teile in Funktionen auslagern\n",
    "import functions as fct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wt103/itos_wt103.pkl', 'rb') as pickle_file:\n",
    "    itos_wiki = pickle.load(pickle_file)\n",
    "    \n",
    "stoi_wiki = collections.defaultdict(lambda:-1, {v:k for k,v \n",
    "                                              in enumerate(itos_wiki)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'Tokens': ['\\n', 'xbos', 'yesterday', 'i', 'flew', 'with', 'a', 'new', 'airline', 'and', 'it', 'was', 'really'], 'Encoded_Tokens': [288, 303, 13256, 74, 3273, 19, 9, 58, 4745, 6, 29, 12, 1041]}\n"
     ]
    }
   ],
   "source": [
    "from functions import Prep\n",
    "\n",
    "sent = 'Yesterday I flew with a new airline and it was really'\n",
    "tok_sent = Prep.tokenize(sent, stoi_wiki)\n",
    "print(tok_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13, 238462)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = Prep.OneHot(tok_sent['Encoded_Tokens'], dimension = len(itos_wiki))\n",
    "print(onehot)\n",
    "onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.4396  -0.39229 -0.12621 ...  0.12117  0.2433   0.10483]\n",
      " [-0.16437 -0.35955 -0.04734 ...  0.04385  1.08976 -0.09999]\n",
      " [ 0.28111 -0.59665  0.1551  ... -0.10522  0.31061  0.24847]\n",
      " ...\n",
      " [-0.10496  0.75285 -0.14176 ...  0.06853  0.02307  0.0305 ]\n",
      " [ 0.03717  0.86379 -0.01512 ... -0.01448 -0.64909 -0.00839]\n",
      " [ 0.6857  -0.79381 -0.18487 ... -0.30761  0.44434 -0.31734]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13, 400)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_wgts_wiki = torch.load('wt103/fwd_wt103.h5', map_location=lambda storage, \n",
    "                  loc: storage)\n",
    "\n",
    "embedding = np.matmul(onehot, lm_wgts_wiki['0.encoder.weight'])\n",
    "print(embedding)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.03709  0.19475 -0.01835 ... -0.00458  0.13194  0.01884]\n",
      " [-0.20202  0.13205 -0.01257 ... -0.01585  0.15082  0.10082]\n",
      " [-0.28976  0.1191  -0.10623 ... -0.10468 -0.30108  0.51579]\n",
      " ...\n",
      " [ 0.05157  0.11202  0.00307 ... -0.39102 -0.0455  -0.00869]\n",
      " [-0.0446   0.05912 -0.02197 ... -0.37411  0.04055 -0.33509]\n",
      " [-0.0128   0.10315 -0.11878 ... -0.39192 -0.02202 -0.03815]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13, 400)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functions import LSTM\n",
    "lstm  = LSTM(embedding, lm_wgts_wiki, 1150)\n",
    "st_lstm = LSTM.stacked(lstm)\n",
    "print(st_lstm.hidden_state_l2)\n",
    "st_lstm.hidden_state_l2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n xbos yesterday i flew with a new airline and it was really'] \n",
      " [           word  probability\n",
      "9             a     0.000013\n",
      "2           the     0.000010\n",
      "42          not     0.000009\n",
      "3             ,     0.000005\n",
      "8            to     0.000004\n",
      "415        good     0.000004\n",
      "1077     likely     0.000004\n",
      "587   important     0.000004\n",
      "284       great     0.000003\n",
      "127        well     0.000003]\n"
     ]
    }
   ],
   "source": [
    "from functions import Prediction\n",
    "prediction = Prediction.predict(st_lstm.hidden_state_l2, lm_wgts_wiki, stoi_wiki, tok_sent, 10)\n",
    "print(prediction[0], \"\\n\", prediction[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[-0.00927  0.05593 -0.01051 ... -0.00032  0.09934  0.00069]\n",
      " [-0.02445  0.08898 -0.05619 ...  0.0116   0.05344  0.03297]\n",
      " [ 0.01426  0.07458 -0.00367 ... -0.01137 -0.09863  0.12703]\n",
      " ...\n",
      " [-0.09402  0.06873 -0.01269 ... -0.09954 -0.07107  0.02303]\n",
      " [-0.05729  0.06447 -0.05503 ... -0.18477  0.00308 -0.1477 ]\n",
      " [ 0.00681  0.03628 -0.2629  ... -0.16824 -0.09854  0.03765]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13, 400)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading class Classifier from functions notebook\n",
    "from functions import Classifier\n",
    "\n",
    "# loading the twitter dataset vocabulary\n",
    "with open('lm/itos.pkl', 'rb') as pickle_file:\n",
    "    itos = pickle.load(pickle_file)\n",
    "\n",
    "stoi = collections.defaultdict(lambda:0, \n",
    "                               {v:k for k,v in enumerate(itos)})\n",
    "\n",
    "# loading the training model parameters of the classifier\n",
    "clas_wgts = torch.load('models/clas_2.h5', map_location=lambda storage, \n",
    "                  loc: storage)\n",
    "\n",
    "# prep functions executes tokenization, one-hot encoding, embedding\n",
    "# and lstm forward pass, outputs hidden state tensor of last lstm\n",
    "hidden_state = Classifier.prep('Yesterday I flew with a new airline and it was well', clas_wgts, stoi)\n",
    "print(hidden_state)\n",
    "hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00681  0.03628 -0.2629  ... -0.12163 -0.03254  0.00786]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1200,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply concat pooling for last hidden state vector (representing last word of tweet)\n",
    "concat = Classifier.concat_pooling(hidden_state)\n",
    "print(concat)\n",
    "concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.      0.      1.38399 2.61893 0.01491 1.84187 0.      0.      1.0487  0.      0.      1.29328 0.\n",
      "  0.      3.78879 2.94374 0.9839  0.      0.      1.80533 2.62585 0.      1.06425 0.      0.94657 0.4659\n",
      "  0.      0.12778 0.      0.      0.      1.04623 0.      0.      0.      2.42056 0.      0.      1.62496\n",
      "  0.      2.32776 0.      2.13438 0.44674 0.      1.20542 2.44793 1.09395 0.      0.     ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 50)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process concat pooled hidden state in relu layer\n",
    "relu = Classifier.relu_layer(concat, clas_wgts)\n",
    "print(relu)\n",
    "relu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 0.002050045832835973, 'neutral': 0.022765535247444953, 'positive': 0.975184418919719}\n"
     ]
    }
   ],
   "source": [
    "# process output from relu layer in softmax layer and print out prediction\n",
    "prediction = Classifier.clas_predict(relu, clas_wgts)\n",
    "print(prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
