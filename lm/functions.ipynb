{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "!pip install fastai==0.7.0\n",
    "!pip install torchtext==0.2.3\n",
    "\n",
    "import fastai\n",
    "from fastai.imports import *\n",
    "from fastai.torch_imports import *\n",
    "from fastai.core import *\n",
    "from fastai.model import fit\n",
    "from fastai.dataset import *\n",
    "\n",
    "import torchtext\n",
    "from torchtext import vocab, data\n",
    "from torchtext.datasets import language_modeling\n",
    "\n",
    "from fastai.rnn_reg import *\n",
    "from fastai.rnn_train import *\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "from fastai.text import *\n",
    "\n",
    "import dill as pickle\n",
    "!pip install spacy\n",
    "!python -m spacy download en\n",
    "import spacy\n",
    "import html\n",
    "\n",
    "import pickle\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prep:\n",
    "    \n",
    "    def __init__(self, tweet, label, vocab):\n",
    "        self.tweet = tweet\n",
    "        self.label = label\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    # cleaning function\n",
    "    def cleaning(x):\n",
    "        re1 = re.compile(r'  +')\n",
    "        x = x.replace('#','').replace('&amp;', '&')\n",
    "        #return re1.sub(' ', html.unescape(x))\n",
    "        return re1.sub(' ', re.sub('https?://[A-Za-z0-9./]+', '',html.unescape(x)))\n",
    "    \n",
    "    # tokenizer function\n",
    "    def get_texts(df, n_lbls=1):\n",
    "        BOS = 'xbos'\n",
    "        labels = np.unique(df.iloc[:,range(n_lbls)].values, return_inverse=True)[n_lbls]\n",
    "        texts = f'\\n{BOS} ' + df[n_lbls].astype(str)\n",
    "        for i in range(n_lbls+1, len(df.columns)): \n",
    "            texts += df[i].astype(str)\n",
    "        texts = texts.apply(Prep.cleaning).values.astype(str)\n",
    "        tok = Tokenizer().proc_all_mp(partition_by_cores(texts))\n",
    "        return tok, list(labels)\n",
    "        \n",
    "    # iterator function\n",
    "    def get_all(df, n_lbls):\n",
    "        tok, labels = [], []\n",
    "        for i, r in enumerate(df):\n",
    "            print(i)\n",
    "            tok_, labels_ = Prep.get_texts(r, n_lbls)\n",
    "            tok += tok_;\n",
    "            labels += labels_\n",
    "        return tok, labels\n",
    "    \n",
    "    # function to automatically tokenize single tweet\n",
    "    def tokenize(tweet, vocab, label = '0', chunksize = 1,\n",
    "                 folder_name = 'Test_lm', file_name = 'text'): \n",
    "        \n",
    "        text = np.array(pd.Series(tweet))\n",
    "        labels = np.array(pd.Series(label))\n",
    "\n",
    "        colNames = ['labels','text']\n",
    "        textdf = pd.DataFrame({'text':text, 'labels':labels}, columns = colNames)\n",
    "    \n",
    "        textdf.to_csv(folder_name+'/'+file_name+'.csv', header=False, index=False)\n",
    "    \n",
    "        BOS = 'xbos'\n",
    "        textdf1 = pd.read_csv(folder_name+'/'+file_name+'.csv', header=None, chunksize=chunksize)\n",
    "        TextLm = Prep.get_all(textdf1, 1)\n",
    "        TextLm = (TextLm[0])\n",
    "        \n",
    "        for i in enumerate(TextLm[0]):\n",
    "            if TextLm[0][i[0]] not in vocab.keys():\n",
    "                TextLm[0][i[0]] = '_unk_'\n",
    "        \n",
    "        tok = [[vocab[o] for o in p] for p in TextLm]\n",
    "        tok = tok[0]\n",
    "    \n",
    "        output = {\n",
    "            \"Tokens\": TextLm[0],\n",
    "            \"Encoded_Tokens\": tok\n",
    "        }\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    # \n",
    "    def OneHot(sequences, dimension):\n",
    "        results = np.zeros((len(sequences), dimension))\n",
    "        for i, sequence in enumerate(sequences):\n",
    "            results[i, sequence] = 1\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def relu(x):\n",
    "    relu = np.maximum(0,x)\n",
    "    return relu\n",
    "    \n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \n",
    "    def __init__(self, input, wgts, nh):\n",
    "        \n",
    "        self.input = input\n",
    "        self.wgts = wgts\n",
    "        self.nh = nh\n",
    "        self.em_sz = []\n",
    "        self.hidden_state_l0 = []\n",
    "        self.hidden_state_l1 = []\n",
    "        self.hidden_state_l2 = []\n",
    "        self.cell_state_l0 = []\n",
    "        self.cell_state_l1 = [] \n",
    "        self.cell_state_l2 = [] \n",
    "        \n",
    "    # single lstm layer\n",
    "    def single(input, wgts, nh, stage = '0'):\n",
    "        \n",
    "        # input weights and bias from the loaded torch model, converted into numpy variables\n",
    "        wii = np.matrix(wgts['0.rnns.'+stage+'.module.weight_ih_l0'][:nh].numpy())\n",
    "        wif = np.matrix(wgts['0.rnns.'+stage+'.module.weight_ih_l0'][nh:2*nh].numpy())\n",
    "        wig = np.matrix(wgts['0.rnns.'+stage+'.module.weight_ih_l0'][2*nh:3*nh].numpy())\n",
    "        wio = np.matrix(wgts['0.rnns.'+stage+'.module.weight_ih_l0'][3*nh:4*nh].numpy())\n",
    "    \n",
    "        bii = np.matrix(wgts['0.rnns.'+stage+'.module.bias_ih_l0'][:nh].numpy())\n",
    "        bif = np.matrix(wgts['0.rnns.'+stage+'.module.bias_ih_l0'][nh:2*nh].numpy())\n",
    "        big = np.matrix(wgts['0.rnns.'+stage+'.module.bias_ih_l0'][2*nh:3*nh].numpy())\n",
    "        bio = np.matrix(wgts['0.rnns.'+stage+'.module.bias_ih_l0'][3*nh:4*nh].numpy())\n",
    "        \n",
    "        # output weights and bias from the loaded torch model, converted into numpy variables\n",
    "        whi = np.matrix(wgts['0.rnns.'+stage+'.module.weight_hh_l0_raw'][:nh].numpy())\n",
    "        whf = np.matrix(wgts['0.rnns.'+stage+'.module.weight_hh_l0_raw'][nh:2*nh].numpy())\n",
    "        whg = np.matrix(wgts['0.rnns.'+stage+'.module.weight_hh_l0_raw'][2*nh:3*nh].numpy())\n",
    "        who = np.matrix(wgts['0.rnns.'+stage+'.module.weight_hh_l0_raw'][3*nh:4*nh].numpy())\n",
    "    \n",
    "        bhi = np.matrix(wgts['0.rnns.'+stage+'.module.bias_hh_l0'][:nh].numpy())\n",
    "        bhf = np.matrix(wgts['0.rnns.'+stage+'.module.bias_hh_l0'][nh:2*nh].numpy())\n",
    "        bhg = np.matrix(wgts['0.rnns.'+stage+'.module.bias_hh_l0'][2*nh:3*nh].numpy())\n",
    "        bho = np.matrix(wgts['0.rnns.'+stage+'.module.bias_hh_l0'][3*nh:4*nh].numpy())\n",
    "    \n",
    "        hs, cs = np.zeros(nh), np.zeros(nh)\n",
    "        hidden_matrix = np.empty((0,nh))\n",
    "        cell_matrix = np.empty((0,nh))\n",
    "        \n",
    "        ## LSTM Process:\n",
    "        # vectors for ignore gate, forget gate, cell gate, output gate, cell state and hidden state\n",
    "        # are calculated and updated per loop for every word vector. Cell states and hidden states \n",
    "        # are all stored\n",
    "        for t,v in enumerate(input):\n",
    "            ig = sigmoid(np.matmul(wii,input[t]) + bii + np.matmul(hs,whi) + bhi)\n",
    "            fg = sigmoid(np.matmul(wif,input[t]) + bif + np.matmul(hs,whf) + bhf)\n",
    "            cg = np.tanh(np.matmul(wig,input[t]) + big + np.matmul(hs,whg) + bhg)\n",
    "            og = sigmoid(np.matmul(wio,input[t]) + bio + np.matmul(hs,who) + bho)\n",
    "            cs = np.multiply(fg,cs) + np.multiply(ig,cg)\n",
    "            hs = np.multiply(og,np.tanh(cs))\n",
    "            hidden_matrix = np.append(hidden_matrix, hs, axis=0)\n",
    "            cell_matrix = np.append(cell_matrix, cs, axis=0)\n",
    "        \n",
    "        hidden_state = np.array(hidden_matrix)\n",
    "        cell_state = np.array(cell_matrix)\n",
    "        return hidden_state, cell_state\n",
    "    \n",
    "    # stacked consisting of three layers\n",
    "    def stacked(self):\n",
    "        \n",
    "        self.em_sz = len(self.input[0])\n",
    "        # First LSTM Layer, nh = 1150\n",
    "        hidden_0 = LSTM.single(input = self.input, wgts = self.wgts, nh = self.nh, stage = '0')\n",
    "        # store hidden states and cell states into class object\n",
    "        self.hidden_state_l0, self.cell_state_l0 = hidden_0[0], hidden_0[1]\n",
    "        \n",
    "        # Second LSTM Layer, nh = 1150\n",
    "        hidden_1 = LSTM.single(input = hidden_0[0], wgts = self.wgts, nh = self.nh, stage = '1')\n",
    "        #store hidden states and cell states into class object\n",
    "        self.hidden_state_l1, self.cell_state_l1 = hidden_1[0], hidden_1[1]\n",
    "        \n",
    "        # Third LSTM Layer, nh = 400 = Embedding Size as LSTM Output\n",
    "        hidden_2 = LSTM.single(input = hidden_1[0], wgts = self.wgts, nh = self.em_sz, stage = '2')\n",
    "        # store hidden states and cell states into class object\n",
    "        self.hidden_state_l2, self.cell_state_l2 = hidden_2[0], hidden_2[1]\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Prediction:\n",
    "    \n",
    "    #\n",
    "    def pred_sentence(input, pos):\n",
    "        sent = ' '.join(word for word in input['Tokens'][:pos])\n",
    "        sent += str(\" \") + input['Tokens'][pos]\n",
    "        return sent\n",
    "\n",
    "    def predict(input, model, vocab, tweet, pred_length = 10):\n",
    "        pred = softmax(np.matmul(input, np.transpose(model['1.decoder.weight'])))\n",
    "        pred_vec = pred[-1]\n",
    "        pred_tok = pred_vec.argsort()[::-1][:pred_length]\n",
    "        vs = len(vocab)\n",
    "        stoi_df = pd.DataFrame(list(vocab.keys())[:vs], list(vocab.values())[:vs], columns = ['word'])\n",
    "        stoi_df['probability'] = list(pred_vec)\n",
    "        output = stoi_df.iloc[pred_tok]\n",
    "        sentence = Prediction.pred_sentence(tweet, -1)\n",
    "        return [[sentence], [output]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier:\n",
    "    \n",
    "    # prepare tweet for demonstration of classifier\n",
    "    def prep(tweet, wgts, vocab, nh = 1150):\n",
    "        testtweet = Prep.tokenize(tweet, vocab)\n",
    "        onehot    = Prep.OneHot(testtweet['Encoded_Tokens'], dimension = len(vocab))\n",
    "        embedding = np.matmul(onehot, wgts['0.encoder.weight'])\n",
    "        lstm      = LSTM(embedding, wgts, nh)\n",
    "        st_lstm   = LSTM.stacked(lstm)\n",
    "        hidden    = st_lstm.hidden_state_l2\n",
    "        return hidden\n",
    "    \n",
    "    # concatenate last, maxpooled and mean hidden state\n",
    "    def concat_pooling(input):\n",
    "        maxpool  = np.max(input, axis = 0)\n",
    "        meanpool = np.mean(input, axis = 0)\n",
    "        concat   = np.concatenate((input[-1], maxpool, meanpool), axis = 0)\n",
    "        return concat \n",
    "    \n",
    "    def relu_layer(input, wgts):\n",
    "        # pulling model parameters for ReLU layer\n",
    "        relu_wgts = np.matrix(wgts['1.layers.0.lin.weight'].numpy())\n",
    "        relu_bias = np.matrix(wgts['1.layers.0.lin.bias'].numpy())\n",
    "        # relu calculation\n",
    "        output    = relu(np.matmul(relu_wgts, input) + relu_bias)\n",
    "        return output\n",
    "    \n",
    "    def clas_predict(input, wgts):\n",
    "        # pulling model parameters for softmax layer\n",
    "        softmax_wgts = np.matrix(wgts['1.layers.1.lin.weight'].numpy())\n",
    "        softmax_bias = np.array(wgts['1.layers.1.lin.bias'].numpy())\n",
    "        # softmax calculation\n",
    "        pred         = softmax(np.transpose(np.array(np.matmul(softmax_wgts, np.transpose(input)))) + softmax_bias)\n",
    "        \n",
    "        predict = {\n",
    "            \"negative\": pred[0][0],\n",
    "            \"neutral\": pred[0][1],\n",
    "            \"positive\": pred[0][2]\n",
    "        }\n",
    "        return predict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to show the effect of the freezing\n",
    "def show_freezing(wgts):\n",
    "    \n",
    "    output = pd.DataFrame()\n",
    "    # from embedding and decoder matrix and from input weights of three lstms the first line is printed\n",
    "    output['embedding'] = wgts['0.encoder.weight'][0][0:10]\n",
    "    output['lstm_0']    = wgts['0.rnns.0.module.weight_ih_l0'][0][0:10]\n",
    "    output['lstm_1']    = wgts['0.rnns.1.module.weight_ih_l0'][0][0:10]\n",
    "    output['lstm_2']    = wgts['0.rnns.2.module.weight_ih_l0'][0][0:10]\n",
    "    output['decoder']   = wgts['1.decoder.weight'][0][0:10]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_loader(path, trn_clas, val_clas, trn_labels, val_labels, bs, pad_idx):\n",
    "    \n",
    "    # fastai class TextDataset stores numpy ndarray with encoded tweets\n",
    "    # and array with labels into one object\n",
    "    trn_ds = TextDataset(trn_clas, trn_labels)\n",
    "    val_ds = TextDataset(val_clas, val_labels)\n",
    "\n",
    "    # fastai samplers\n",
    "    trn_samp = SortishSampler(trn_clas, key=lambda x: len(trn_clas[x]), \n",
    "                              bs=bs)\n",
    "    val_samp = SortSampler(val_clas, key=lambda x: len(val_clas[x]))\n",
    "    \n",
    "    # fastai data loader\n",
    "    trn_dl = DataLoader(trn_ds, bs, transpose=True, num_workers=1,\n",
    "                        pad_idx=1, sampler=trn_samp)\n",
    "    val_dl = DataLoader(val_ds, bs, transpose=True, num_workers=1, \n",
    "                        pad_idx=1, sampler=val_samp)\n",
    "    \n",
    "    # fastai ModelData stores all model inputs provided by the data loader\n",
    "    md = ModelData(path , trn_dl, val_dl)\n",
    "    return md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
